{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b25071d9",
   "metadata": {},
   "source": [
    "### 02. Neural Network classification with PyTorch\n",
    "Classification is a problem of predicting whether something is one thing or another (there can be multiple things as the options)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27b879",
   "metadata": {},
   "source": [
    "#### 1. Make classification data and get it ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Make 1000 samples\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "x,y = make_circles(n_samples, noise=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f30813",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'First 5 samples of x:\\n {x[:5]}')\n",
    "print(f'First 5 samples of y:\\n {y[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39917c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame of circle data\n",
    "import pandas as pd\n",
    "\n",
    "circles = pd.DataFrame({\"X1\": x[:, 0], \"X2\": x[:, 1], \"label\": y})\n",
    "circles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize, # Visualize, # Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=x[:,0],\n",
    "            y=x[:,1],\n",
    "            c=y,\n",
    "            cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf83460",
   "metadata": {},
   "source": [
    "### 1.1 Check input and output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first example of features and labels\n",
    "x_sample = x[0]\n",
    "y_sample = y[0]\n",
    "\n",
    "print(f\"Values for one sample of x: {x_sample} and the same for y: {y_sample}\")\n",
    "print(f\"Shapes for one sample of x: {x_sample.shape} and the same for y: {y_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d370a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into tensors\n",
    "import torch\n",
    "x = torch.from_numpy(x).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "x[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x), x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fa57d",
   "metadata": {},
   "source": [
    "### 2. Building a model\n",
    "\n",
    "Let's build a model to classify our blue and red dots.\n",
    "\n",
    "To do so, we want to:\n",
    "1. Setup device agnostic code so our code will run on an accelerator (GPU) if there is one.\n",
    "2. Construct a model (by subclassing `nn.Module`)\n",
    "3. Define a loss function and optimizer.\n",
    "4. Create a training and test loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f8075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and nn\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Make device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train # check if data is available on target device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe15008",
   "metadata": {},
   "source": [
    "Now we've setup devide agnostic code, let's create a model that:\n",
    "\n",
    "1. Subclasses `nn.Module` (almost all models in PyTorch subclass `nn.Module`)\n",
    "2. Create 2 `nn.Linear()` layers that are capable of handling the shapes of our data\n",
    "3. Defines a `forward()` method that outlines the forward pass (or forward computation) of the model\n",
    "4. Instantiate an instance of our model class and send it to the target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2860b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Construct a model that subclasses nn.Module\n",
    "class CircleModelv0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2. Create 2 nn.Linear layers capable of handling the shapes of our data\n",
    "        self.layer_1 = nn.Linear(\n",
    "            in_features=2, out_features=5\n",
    "        )  # takes in 2 features and upscales to 5 features\n",
    "        self.layer_2 = nn.Linear(\n",
    "            in_features=5, out_features=1\n",
    "        )  # takes in 5 features from previous layer and outputs a single feature (same shape as y)\n",
    "\n",
    "    # 3. Define a forward() method that outlines the forward pass\n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.layer_1(x))  # x -> layer_1 -> layer_2 -> output\n",
    "\n",
    "\n",
    "# 4. Instantiate an instance of out model class and send it to the target device\n",
    "model_0 = CircleModelv0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model_0.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replicate the model above using nn.Sequential()\n",
    "\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model_0.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9eaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951806f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "with torch.inference_mode():\n",
    "    untrained_preds = model_0(x_test.to(device))\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(x_test)}, Shape: {x_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n{torch.round(untrained_preds[:10])}\")\n",
    "print(f\"\\nFirst 10 labels:\\n{y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8516e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[:10], y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf3963",
   "metadata": {},
   "source": [
    "### 2.1 Setup loss function and optimizer\n",
    "Which loss function or optimizer should you use?\n",
    "\n",
    "Again... this is problem specific.\n",
    "For example for regression you might want MAE or MSE (mean absolute error or mean squared error)\n",
    "\n",
    "For classification you might want binary cross entropy or categorical cross entropy (cross entropy).\n",
    "\n",
    "As a reminder, the loss function measures how *wrong* your model's predictions are.\n",
    "\n",
    "And for optimizers, two of the most common and useful are SGD and Adam, however PyTorch has many built-in options.\n",
    "\n",
    "* For the loss function we're going to use `torch.nn.BECWithLogitsLoss()`\n",
    "* For different optimizers see `torch.optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd7a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid activation functiopn built-in\n",
    "\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy - out of 100 examples, what percentage does our model get right ?\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct/len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663784e",
   "metadata": {},
   "source": [
    "### 3. Train model\n",
    "To train our model, we're going to need to build a training loop with the following steps:\n",
    "\n",
    "1. Forward pass\n",
    "2. Calculate the loss\n",
    "3. Optimize zero grad\n",
    "4. Loss backward (backpropagation)\n",
    "5. Optimizer step (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a9295",
   "metadata": {},
   "source": [
    "3.1 Going from raw logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "Out model outputs are going to be raw **logits**.\n",
    "\n",
    "We can convert these **logits** into prediction probabilities by passing them to some kind of activation function (e.g. sigmoid for binary crossentropy and softmax for multiclass classification).\n",
    "\n",
    "Then we can convert our model's prediction to **prediction labels** by either rounding them or taking the `argmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52824b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 outputs of the forward pass on the test data\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_0(x_test.to(device))[:5]\n",
    "y_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5408606",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7363e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sigmoid activation function on our model logits to turn them into prediction probabilities\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "y_pred_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7899083",
   "metadata": {},
   "source": [
    "For our prediction probabilities values, we need to perform a range-style rounding on them:\n",
    "\n",
    "* `y_pred_probs` >= 0.5, `y=1` (class 1)\n",
    "* `y_pred_probs` < 0.5, `y=0` (class 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the predicted labels\n",
    "y_preds = torch.round(y_pred_probs)\n",
    "\n",
    "# In full (logits -> prod probs -> pred labels)\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(x_test.to(device))[:5]))\n",
    "\n",
    "# Check for equality\n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
    "\n",
    "# Get rid of extra dimension\n",
    "y_preds.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77779d8c",
   "metadata": {},
   "source": [
    "### 3.2 Building a training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put data to the target device\n",
    "x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = model_0(x_train).squeeze()\n",
    "    y_preds = torch.round(\n",
    "        torch.sigmoid(y_logits)\n",
    "    )  # turn logits -> pred probs -> pred labels\n",
    "\n",
    "    # 2. Calculate the loss/accuracy\n",
    "    loss = loss_fn(\n",
    "        y_logits, y_train\n",
    "    )  # nn.BCEWithLogitsLoss expects raw logits as input\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_preds)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step (gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_0(x_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate test loss/acc\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "        \n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss}, Acc: {acc: .2f}% | Test loss: {test_loss: .5f}, Test acc: {test_acc: .2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d6d68",
   "metadata": {},
   "source": [
    "### 4. Make predictions and evaluate the model\n",
    "\n",
    "From the metrics it looks like our model isn't learning anything...\n",
    "\n",
    "So to inspect it let's make some predictions and make them visual!\n",
    "\n",
    "In other words, \"Visualize, visualize, visualize\"\n",
    "\n",
    "To do so, we're going to import a function called `plot_decision_boundary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if it's not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading helper_functions.py\")\n",
    "    requests = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py\")\n",
    "    with open(\"helper_functions.py\", \"wb\") as f:\n",
    "        f.write(requests.content)\n",
    "\n",
    "from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_0, x_train, y_train)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_0, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095e74d",
   "metadata": {},
   "source": [
    "### 5. Improving a model (from a model perspective)\n",
    "* Add more layers - give the model mode chance to learn about patterns in the data\n",
    "* Add more hidden units - go from 5 hidden units to 10 hidden units\n",
    "* Fit for longer\n",
    "* Changing the activation functions\n",
    "* Change the learning rate\n",
    "* Change the loss function\n",
    "\n",
    "These options are all from a model's perspective because they deal directly with the model, rather than the data.\n",
    "\n",
    "And because these options are all values we (as machine learning engineer and data scientist) can change, they are referred to **hyperparameters**\n",
    "\n",
    "Let's try and improve our model by:\n",
    "* Adding more hidden units: 5 -> 10\n",
    "* Increate the number of layers: 2 -> 3\n",
    "* Increate the number of epochs: 100 -> 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c12fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleModelv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # z = self.layer_1(x)\n",
    "        # z = self.layer_2(z)\n",
    "        # z = self.layer_3(z)\n",
    "        # return z\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
    "    \n",
    "model_1 = CircleModelv1().to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06db24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a training and evaluation loop for model_1\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Train for longer\n",
    "epochs = 1000\n",
    "\n",
    "# Put data on the target device\n",
    "(\n",
    "    x_train,\n",
    "    y_train,\n",
    ") = x_train.to(\n",
    "    device\n",
    "), y_train.to(device)\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = model_1(x_train).squeeze()\n",
    "    y_preds = torch.round(\n",
    "        torch.sigmoid(y_logits)\n",
    "    )  # logits -> prediction probability -> prediction label\n",
    "\n",
    "    # 2. Calculate the loss/acc\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step (gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_1(x_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "        # Print out what's happening\n",
    "        if epoch % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch} | Loss: {loss: .5f}, Acc: {acc: .2f} | Test loss: {test_loss: .5f}, test_acc: {test_acc: .2f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_1, x_train, y_train) # model_1 = no non-linearity\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_1, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd792af",
   "metadata": {},
   "source": [
    "### 5.1 Preparing data to see if our model can fit a straight line\n",
    "One way to troubleshoot to a larger problem is to test out a smaller problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some data (same as workflow)\n",
    "\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.01\n",
    "\n",
    "# Create data\n",
    "x_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y_regression = weight * x_regression + bias # Linear regression formula (without epsilon)\n",
    "\n",
    "# Check the data\n",
    "print(len(x_regression))\n",
    "x_regression[:5], y_regression[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b36f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test splits\n",
    "train_split = int(0.8 * len(x_regression))\n",
    "x_train_regression, y_train_regression = x_regression[:train_split], y_regression[:train_split]\n",
    "x_test_regression, y_test_regression = x_regression[train_split:], y_regression[train_split:]\n",
    "\n",
    "# Check the lengths of each\n",
    "len(x_train_regression), len(x_test_regression), len(y_train_regression), len(y_test_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27935a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(train_data=x_train_regression, train_labels=y_train_regression, test_data=x_test_regression, test_labels=y_test_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d372d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same architecture as model_1 (but using nn.Sequential())\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(in_features = 1, out_features = 10),\n",
    "    nn.Linear(in_features=10, out_features=10),\n",
    "    nn.Linear(in_features=10, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Put the data on the target device\n",
    "x_train_regression, y_train_regression = x_train_regression.to(\n",
    "    device\n",
    "), y_train_regression.to(device)\n",
    "x_test_regression, y_test_regression = x_test_regression.to(\n",
    "    device\n",
    "), y_test_regression.to(device)\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    y_preds = model_2(x_train_regression)\n",
    "    loss = loss_fn(y_preds, y_train_regression)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Testing\n",
    "    model_2.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_2(x_test_regression)\n",
    "        test_loss = loss_fn(test_pred, y_test_regression)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss: .5f} | Test loss: {test_loss: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14497867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on evaluation mode\n",
    "model_2.eval()\n",
    "\n",
    "# Make predictions (inference)\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_2(x_test_regression)\n",
    "\n",
    "# Plot data and predictions\n",
    "plot_predictions(\n",
    "    train_data=x_train_regression.cpu(),\n",
    "    train_labels=y_train_regression.cpu(),\n",
    "    test_data=x_test_regression.cpu(),\n",
    "    test_labels=y_test_regression.cpu(),\n",
    "    predictions=y_preds.cpu()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc807b",
   "metadata": {},
   "source": [
    "### 6. The missing piece: non-linearity\n",
    "\n",
    "\"What patterns could you draw if you were given an infinite amount of a straight and non-straight lines?\"\n",
    "\n",
    "Or in machine learning terms, an infinite (but really it's finite) of linear and non-linear functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206ebad",
   "metadata": {},
   "source": [
    "### 6.1 Recreating non-linear data (red and blue circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and plot data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "x, y = make_circles(n_samples, noise=0.03, random_state=42)\n",
    "\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eadaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors and then to train and test splits\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Turn data into tensors\n",
    "x = torch.from_numpy(x).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# Split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)\n",
    "\n",
    "x_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66b693",
   "metadata": {},
   "source": [
    "### 6.2 Building a model with non-linearity\n",
    "* Linear = straight line\n",
    "* Non-linear = non-straight line\n",
    "\n",
    "Artificial neural network are a large combination of linear (straight) and non-straight (non-linear) functions which are potentially able to find patterns in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1fdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with non-linear activation function\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CircleModelv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Where should we put our non-linear activation functions?\n",
    "        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
    "    \n",
    "model_3 = CircleModelv2().to(device)\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe733d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e388d2",
   "metadata": {},
   "source": [
    "### 6.3 Training a model with non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61786cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Put all data on the target device\n",
    "x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop through data\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_3.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = model_3(x_train).squeeze()\n",
    "    y_pred = torch.round(\n",
    "        torch.sigmoid(y_logits)\n",
    "    )  # logits -> predictions probabilities -> prediction labels\n",
    "\n",
    "    # 2. Calculate the loss\n",
    "    loss = loss_fn(\n",
    "        y_logits, y_train\n",
    "    )  # BCEWithLogitsLoss (takes in logits as first input)\n",
    "\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Step the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_3.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model_3(x_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch} | Loss: {loss: .4f}, Acc: {acc: .2f}% | Test Loss: {test_loss: .4f}, Test Acc: {test_acc: .2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b66da",
   "metadata": {},
   "source": [
    "### 6.4 Evaluating a model trained with non-linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e184406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions\n",
    "model_3.eval()\n",
    "with torch.inference_mode():\n",
    "    y_preds = torch.round(torch.sigmoid(model_3(x_test))).squeeze()\n",
    "    \n",
    "y_preds[:10], y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f070e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_1, x_train, y_train) # model_1 = no non-linearity\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_3, x_test, y_test) # model_3 = has non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d9b23",
   "metadata": {},
   "source": [
    "### 7. Replicating non-linear activation functions\n",
    "\n",
    "Neural networks, rather than us telling the model what to learn, we give it the tools to discover patterns in data and it tries to figure out the patterns on its own.\n",
    "\n",
    "And these tools are linear & non-linear functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed86d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor\n",
    "A = torch.arange(-10,10,1,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tensor\n",
    "plt.plot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8fdb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do the same for sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / 1 + torch.exp(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.sigmoid(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigmoid(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c8f12",
   "metadata": {},
   "source": [
    "### 8. Putting it all together with a multi-class classification problem\n",
    "\n",
    "* Binary classification = one thing or another (cat vs dog, spam vs not spam, fraud or not fraud)\n",
    "* Multi-class classification = more than one thing or another (cat vs dog vs chicken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222dfccd",
   "metadata": {},
   "source": [
    "#### 8.1 Creating a toy multi-class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a679f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the hyperparameters for data creation\n",
    "NUM_CLASSES = 4\n",
    "NUM_FEATURES = 2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# 1. Create multi-class data\n",
    "x_blob, y_blob = make_blobs(\n",
    "    n_samples=1000,\n",
    "    n_features=NUM_FEATURES,\n",
    "    centers=NUM_CLASSES,\n",
    "    cluster_std=1.5,  # give the clusters a little shake up\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "# 2. Turn data into tensors\n",
    "x_blob = torch.from_numpy(x_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
    "\n",
    "# 3. Split into train and test\n",
    "x_blob_train, x_blob_test, y_blob_train, y_blob_test = train_test_split(x_blob, y_blob, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# 4. Plot data (visualize, visualize, visualize)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(x_blob[:,0], x_blob[:,1], c=y_blob, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9994cca",
   "metadata": {},
   "source": [
    "### 8.2 Building a multi-class classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class BlobModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        \"\"\"Initializes multi-class classification model\"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "\n",
    "# Create an instance of BlobModel and send it to the target device\n",
    "model_4 = BlobModel(input_features=2, output_features=4, hidden_units=8).to(device)\n",
    "model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e4a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_blob_train.shape, y_blob_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff764a53",
   "metadata": {},
   "source": [
    "### 8.3 Create a loss function and an optimizer for multi-class classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2938e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function for multi-class classification - loss function measures how wrong our predictions are\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an optimizer for multi-class classification - updates our model parameter to try to reduce the loss\n",
    "optimizer = torch.optim.SGD(params=model_4.parameters(), lr=0.1) # learning rate is a hyperparameter you can change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6ec92",
   "metadata": {},
   "source": [
    "### 8.4 Getting prediction probabilities for a multi-class PyTorch model.\n",
    "In order to evaluate and train and test our model, we need to convert our model's outputs (logits) to prediction probabilities and then to prediction labels.\n",
    "\n",
    "Logits (raw output of the model) -> Pred probs (use `torch.softmax`) -> Pred labels (take the argmax of the prediction probilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c386a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some raw outputs of out model (logits)\n",
    "model_4.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_4(x_blob_test.to(device))\n",
    "y_logits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ccb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_blob_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our model's logit outputs to prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "print(y_logits[:5])\n",
    "print(y_pred_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7805c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our model's prediction probabilities to prediction labels\n",
    "y_preds = torch.argmax(y_pred_probs, dim=1)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801758b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_blob_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca902d",
   "metadata": {},
   "source": [
    "#### 8.5 Creating a training loop and testing loop for a multi-class PyTorch model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the multi-class model to the data\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put data to the target device\n",
    "x_blob_train, y_blob_train = x_blob_train.to(device), y_blob_train.to(device)\n",
    "x_blob_test, y_blob_test = x_blob_test.to(device), y_blob_test.to(device)\n",
    "\n",
    "# Loop through data\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_4.train()\n",
    "\n",
    "    y_logits = model_4(x_blob_train)\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "    loss = loss_fn(y_logits, y_blob_train)\n",
    "    acc = accuracy_fn(y_true=y_blob_train, y_pred=y_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_4.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model_4(x_blob_test)\n",
    "        test_preds = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "        test_loss = loss_fn(test_logits, y_blob_test)\n",
    "        test_acc = accuracy_fn(y_true=y_blob_test, y_pred=test_preds)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch} | Loss: {loss: .4f}, Acc: {acc: .2f}% | Test loss: {test_loss: .4f}, Test acc: {test_acc: .2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a12fd0",
   "metadata": {},
   "source": [
    "#### 8.6 Making and evaluating predictions with a PyTorch multi-class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model_4.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_4(x_blob_test)\n",
    "    \n",
    "# View the first 10 predictions\n",
    "y_logits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go from logits -> Prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "y_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go from pred probs to pred labels\n",
    "y_preds = torch.argmax(y_pred_probs, dim=1)\n",
    "y_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_4, x_blob_train, y_blob_train)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_4, x_blob_test, y_blob_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e09833",
   "metadata": {},
   "source": [
    "### 9. A few more classification metrics... (to evaluate our classification model)\n",
    "\n",
    "* Accuracy - out of 100 samples, how many does our model get right?\n",
    "* Precision\n",
    "* Recall\n",
    "* Pl-score\n",
    "* Confusion matrix\n",
    "* Classification report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
