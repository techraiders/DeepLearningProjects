{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32234a72",
   "metadata": {},
   "source": [
    "### Building first PyTorch model!\n",
    " * start with randon values (weight & bias)\n",
    " * look at the training data and adjust the random values to better represent (or get closer to) the ideal values (the weight and bias values we used to create the data)\n",
    "\n",
    "\n",
    "How does it do so?\n",
    "Through two main algorithms:\n",
    "1. Gradient descent\n",
    "2. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22669c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Create a linerar regression model class\n",
    "class LinearRegressionModel(\n",
    "    nn.Module  # <- nn.Module contains all the building blocks for neural network\n",
    "):  # <- almost everything in PyTorch inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initalize model parameters\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(\n",
    "                1,  # <- start with a random weight and try to adjust it to the ideal weight\n",
    "                requires_grad=True,  # <- can this parameter be updated via gradient descent?\n",
    "                dtype=torch.float32,  # PyTorch loves the datatype torch.float32\n",
    "            )\n",
    "        )  # <- PyTorch loves the datatype torch.float32\n",
    "\n",
    "        self.bias = nn.Parameter(\n",
    "            torch.randn(\n",
    "                1,  # <- start with a random bias and try to adjust it to the ideal bias\n",
    "                requires_grad=True,  # <- can this parameter be updated via gradient descent?\n",
    "                dtype=torch.float32,  # PyTorch loves the datatype torch.float32\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Forward method to define the computation in the model, all subclasses of nn.Module need to overwrite forward method.\n",
    "        # This defines the forward computation of the model\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:  # <- \"x\" is the input data\n",
    "            return self.weight * x + self.bias  # this is the linear regression formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdde3a0",
   "metadata": {},
   "source": [
    "### PyTorch model building essentials\n",
    "\n",
    "1. **torch.nn** - contains all of the buildings for computational graphs (a neural network can be considered a computational graph)\n",
    "\n",
    "2. **torch.nn.Parameter** - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us.\n",
    "\n",
    "3. **torch.nn.Module** - The base class for all neural network modules, if you subclass it, you should overwrite forward()\n",
    "\n",
    "4. **torch.optim** - this where the optimizers in PyTorch live, they will help with gradient descent\n",
    "\n",
    "5. **def forward()** - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
